{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103372a3",
   "metadata": {},
   "source": [
    "# Lecture 8: Introduction to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43036d16",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "\n",
    "## **Introduction to Classification**\n",
    "\n",
    "Classification is one of the key tasks in machine learning, which involves assigning new data to one or more predefined categories. It is widely used in various fields such as text analysis, image recognition, medicine, and financial forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Classification?**\n",
    "\n",
    "Classification involves developing a decision function that, based on a set of input features, determines which category (class) an object belongs to. The algorithm learns this function from labeled training data, which contains examples with assigned class labels.\n",
    "\n",
    "### **Types of Classification**\n",
    "\n",
    "1. **Binary Classification**:\n",
    "   - Data is assigned to one of two classes.\n",
    "   - Example: Is an email spam? (Spam/Not Spam)\n",
    "\n",
    "2. **Multiclass Classification**:\n",
    "   - Data can belong to one of many classes.\n",
    "   - Example: Recognizing flower species (setosa, versicolor, virginica).\n",
    "\n",
    "3. **Multilabel Classification**:\n",
    "   - Data can belong to more than one class simultaneously.\n",
    "   - Example: Tagging topics in articles (sports, technology, politics).\n",
    "\n",
    "---\n",
    "\n",
    "## **Examples of Classification Applications**\n",
    "\n",
    "1. **Medicine**:\n",
    "   - Diagnosing diseases based on test results (Healthy/Sick).\n",
    "2. **Text Analysis**:\n",
    "   - Classifying emails (Spam/Not Spam).\n",
    "   - Sentiment analysis (Positive/Negative).\n",
    "3. **Image Recognition**:\n",
    "   - Identifying objects in images (Cat/Dog).\n",
    "4. **Finance**:\n",
    "   - Predicting credit risk (Good/Bad credit score).\n",
    "5. **Engineering and Industry**:\n",
    "   - Detecting machine failures (Working/Not Working).\n",
    "\n",
    "---\n",
    "\n",
    "## **How Does Classification Work?**\n",
    "\n",
    "1. **Training Dataset**:\n",
    "   - The algorithm is provided with a set of examples, where each sample is labeled with a class.\n",
    "\n",
    "2. **Model Building**:\n",
    "   - The algorithm creates rules (a decision function) that allow it to assign new data to the appropriate classes.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - The model is tested on a separate test dataset that was not used during training.\n",
    "\n",
    "4. **Prediction**:\n",
    "   - The model assigns new samples to appropriate classes based on the learned rules.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Classification Algorithms**\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - A classic algorithm for binary classification that predicts the probability of belonging to a particular class.\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN)**:\n",
    "   - A distance-based algorithm that assigns a class based on the nearest neighbors in feature space.\n",
    "\n",
    "3. **Decision Trees**:\n",
    "   - Build a tree structure where each node represents a decision based on data features.\n",
    "\n",
    "4. **Naive Bayes**:\n",
    "   - A probabilistic algorithm assuming feature independence; commonly used in text classification.\n",
    "\n",
    "5. **Support Vector Machines (SVM)**:\n",
    "   - An algorithm that maximizes the margin between classes, effective for complex decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Decision Boundary**\n",
    "\n",
    "A decision boundary is a line (or surface in multidimensional space) that separates different classes. In classification, it is crucial for the decision boundary to be well-suited to the data:\n",
    "- **Overfitting**: The boundary is too closely fitted to the training data, causing the model to fail at generalization.\n",
    "- **Underfitting**: The boundary is too simplistic, ignoring important patterns in the data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09a6959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50405ab8e914f20b523dcf30562a491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Regularization)', max=2.0, min=-2.0), RadioButâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_classification(C=1.0, kernel='rbf', gamma=1.0)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Interactive function for classification\n",
    "def interactive_classification(C=1.0, kernel='rbf', gamma=1.0):\n",
    "    # Train SVM model with specified parameters\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create mesh grid for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and data points\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='o', label='Training Data', edgecolor='k')\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='s', label='Test Data', edgecolor='k')\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f\"SVM Decision Boundary\\nTrain Accuracy: {train_acc:.2f}, Test Accuracy: {test_acc:.2f}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for parameters\n",
    "interact(\n",
    "    interactive_classification,\n",
    "    C=widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='C (Regularization)'),\n",
    "    kernel=widgets.RadioButtons(options=['linear', 'rbf', 'poly'], value='rbf', description='Kernel'),\n",
    "    gamma=widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='Gamma')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f218d4",
   "metadata": {},
   "source": [
    "# **Evaluation Metrics in Classification**\n",
    "\n",
    "Evaluating classification models is crucial to understanding how well a model assigns samples to appropriate classes. Below are the most important metrics used in classification along with their applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Confusion Matrix**\n",
    "\n",
    "The confusion matrix is a table that shows the number of correct and incorrect classifications for each class. It consists of four key values (for binary classification):\n",
    "\n",
    "- **True Positive (TP)**: Samples correctly classified as positive.\n",
    "- **True Negative (TN)**: Samples correctly classified as negative.\n",
    "- **False Positive (FP)**: Samples incorrectly classified as positive (false alarms).\n",
    "- **False Negative (FN)**: Samples incorrectly classified as negative.\n",
    "\n",
    "|                 | Predicted Positive | Predicted Negative |\n",
    "|-----------------|---------------------|---------------------|\n",
    "| **True Positive (TP)** | Number of samples     | -                   |\n",
    "| **False Positive (FP)** | Number of samples     | -                   |\n",
    "| **False Negative (FN)** | -                 | Number of samples       |\n",
    "| **True Negative (TN)** | -                 | Number of samples       |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Accuracy**\n",
    "\n",
    "Accuracy measures the percentage of correctly classified samples in the entire dataset:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- **Advantage**: Simple to understand and calculate.\n",
    "- **Disadvantage**: Insufficient for imbalanced datasets (e.g., 95% samples in one class).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Precision**\n",
    "\n",
    "Precision measures the accuracy of predictions for the positive class:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when minimizing false alarms is crucial (e.g., rare disease classification).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recall (Sensitivity, TPR)**\n",
    "\n",
    "Recall measures the model's ability to detect all samples belonging to the positive class:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when minimizing missed cases is crucial (e.g., fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. F1-Score**\n",
    "\n",
    "F1-Score is the harmonic mean of precision and recall, combining these two metrics into one:\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- **Application**: Useful when balancing precision and recall, especially for imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Specificity (TNR)**\n",
    "\n",
    "Specificity measures the model's ability to correctly identify samples belonging to the negative class:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "- **Application**: Important in scenarios where minimizing false alarms is critical (e.g., security systems).\n",
    "\n",
    "---\n",
    "\n",
    "## **7. NPV (Negative Predictive Value)**\n",
    "\n",
    "NPV measures the percentage of samples classified as negative that truly belong to the negative class:\n",
    "\n",
    "$$\n",
    "\\text{NPV} = \\frac{TN}{TN + FN}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when evaluating the correctness of negative classifications.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. ROC Curve and AUC**\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic Curve)**: A graphical representation of the trade-off between True Positive Rate (Recall) and False Positive Rate (FPR) for different classification thresholds:\n",
    "  $$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$\n",
    "\n",
    "- **AUC (Area Under the Curve)**: A measure of model quality. A value close to 1 indicates a perfect model, while a value of 0.5 suggests a random model.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Precision-Recall (PR) Curve and PR AUC**\n",
    "\n",
    "- **PR Curve**: A graphical representation of the trade-off between Precision and Recall for different classification thresholds. It is especially useful for imbalanced datasets where the positive class is rare.\n",
    "  \n",
    "- **PR AUC (Area Under the Precision-Recall Curve)**: A single-number summary of the PR Curve that evaluates the model's ability to balance Precision and Recall. Higher values indicate better performance.\n",
    "\n",
    "### Key Notes:\n",
    "- Unlike the ROC Curve, the PR Curve focuses on the positive class and provides more meaningful insights for imbalanced datasets.\n",
    "- The curve typically shows a sharp drop in precision as recall increases, indicating the challenge of maintaining precision at higher recall levels.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Log-Loss**\n",
    "\n",
    "Log-Loss measures the certainty of the model's predictions by considering probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Log-Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "- **Application**: Particularly important for probabilistic models such as logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Accuracy** is easy to interpret but not always suitable for imbalanced data.\n",
    "- **Precision** and **Recall** provide a better understanding for datasets with uneven class distributions.\n",
    "- **F1-Score** balances precision and recall effectively.\n",
    "- **Specificity** and **NPV** are particularly important for analyzing the negative class.\n",
    "- **ROC Curve and AUC** assess the model's ability to distinguish between classes across different decision thresholds.\n",
    "- **PR Curve and PR AUC** are essential for evaluating performance on imbalanced datasets and assessing the trade-off between precision and recall.\n",
    "\n",
    "The choice of an appropriate metric depends on the specifics of the problem and which type of error is more costly in the given context.\n",
    "# **Evaluation Metrics in Classification**\n",
    "\n",
    "Evaluating classification models is crucial to understanding how well a model assigns samples to appropriate classes. Below are the most important metrics used in classification along with their applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Confusion Matrix**\n",
    "\n",
    "The confusion matrix is a table that shows the number of correct and incorrect classifications for each class. It consists of four key values (for binary classification):\n",
    "\n",
    "- **True Positive (TP)**: Samples correctly classified as positive.\n",
    "- **True Negative (TN)**: Samples correctly classified as negative.\n",
    "- **False Positive (FP)**: Samples incorrectly classified as positive (false alarms).\n",
    "- **False Negative (FN)**: Samples incorrectly classified as negative.\n",
    "\n",
    "|                 | Predicted Positive | Predicted Negative |\n",
    "|-----------------|---------------------|---------------------|\n",
    "| **True Positive (TP)** | Number of samples     | -                   |\n",
    "| **False Positive (FP)** | Number of samples     | -                   |\n",
    "| **False Negative (FN)** | -                 | Number of samples       |\n",
    "| **True Negative (TN)** | -                 | Number of samples       |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Accuracy**\n",
    "\n",
    "Accuracy measures the percentage of correctly classified samples in the entire dataset:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- **Advantage**: Simple to understand and calculate.\n",
    "- **Disadvantage**: Insufficient for imbalanced datasets (e.g., 95% samples in one class).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Precision**\n",
    "\n",
    "Precision measures the accuracy of predictions for the positive class:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when minimizing false alarms is crucial (e.g., rare disease classification).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recall (Sensitivity, TPR)**\n",
    "\n",
    "Recall measures the model's ability to detect all samples belonging to the positive class:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when minimizing missed cases is crucial (e.g., fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. F1-Score**\n",
    "\n",
    "F1-Score is the harmonic mean of precision and recall, combining these two metrics into one:\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- **Application**: Useful when balancing precision and recall, especially for imbalanced data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Specificity (TNR)**\n",
    "\n",
    "Specificity measures the model's ability to correctly identify samples belonging to the negative class:\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "- **Application**: Important in scenarios where minimizing false alarms is critical (e.g., security systems).\n",
    "\n",
    "---\n",
    "\n",
    "## **7. NPV (Negative Predictive Value)**\n",
    "\n",
    "NPV measures the percentage of samples classified as negative that truly belong to the negative class:\n",
    "\n",
    "$$\n",
    "\\text{NPV} = \\frac{TN}{TN + FN}\n",
    "$$\n",
    "\n",
    "- **Application**: Important when evaluating the correctness of negative classifications.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. ROC Curve and AUC**\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic Curve)**: A graphical representation of the trade-off between True Positive Rate (Recall) and False Positive Rate (FPR) for different classification thresholds:\n",
    "  $$\n",
    "  \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "  $$\n",
    "\n",
    "- **AUC (Area Under the Curve)**: A measure of model quality. A value close to 1 indicates a perfect model, while a value of 0.5 suggests a random model.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Precision-Recall (PR) Curve and PR AUC**\n",
    "\n",
    "- **PR Curve**: A graphical representation of the trade-off between Precision and Recall for different classification thresholds. It is especially useful for imbalanced datasets where the positive class is rare.\n",
    "  \n",
    "- **PR AUC (Area Under the Precision-Recall Curve)**: A single-number summary of the PR Curve that evaluates the model's ability to balance Precision and Recall. Higher values indicate better performance.\n",
    "\n",
    "### Key Notes:\n",
    "- Unlike the ROC Curve, the PR Curve focuses on the positive class and provides more meaningful insights for imbalanced datasets.\n",
    "- The curve typically shows a sharp drop in precision as recall increases, indicating the challenge of maintaining precision at higher recall levels.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Log-Loss**\n",
    "\n",
    "Log-Loss measures the certainty of the model's predictions by considering probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Log-Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "- **Application**: Particularly important for probabilistic models such as logistic regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Accuracy** is easy to interpret but not always suitable for imbalanced data.\n",
    "- **Precision** and **Recall** provide a better understanding for datasets with uneven class distributions.\n",
    "- **F1-Score** balances precision and recall effectively.\n",
    "- **Specificity** and **NPV** are particularly important for analyzing the negative class.\n",
    "- **ROC Curve and AUC** assess the model's ability to distinguish between classes across different decision thresholds.\n",
    "- **PR Curve and PR AUC** are essential for evaluating performance on imbalanced datasets and assessing the trade-off between precision and recall.\n",
    "\n",
    "The choice of an appropriate metric depends on the specifics of the problem and which type of error is more costly in the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf619398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d905163b29a44e21b1ae339ca6865491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='True Positives (TP):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d122935db96e4449aadd31d48d82a2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10, description='False Positives (FP):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e241c382f7ca432ca4b231cd58a5d66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='False Negatives (FN):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a677f0c233174ed783f81dba75b1cf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description='True Negatives (TN):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3343783983d45c9b5bf5eff00b71264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Calculate Metrics', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef622b4c1134d55b48c6b4798643a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "\n",
    "# Widgets for Confusion Matrix Inputs\n",
    "tp_widget = widgets.IntText(value=50, description=\"True Positives (TP):\")\n",
    "fp_widget = widgets.IntText(value=10, description=\"False Positives (FP):\")\n",
    "fn_widget = widgets.IntText(value=20, description=\"False Negatives (FN):\")\n",
    "tn_widget = widgets.IntText(value=100, description=\"True Negatives (TN):\")\n",
    "\n",
    "# Display widgets\n",
    "display(tp_widget, fp_widget, fn_widget, tn_widget)\n",
    "\n",
    "# Button to calculate metrics\n",
    "button = widgets.Button(description=\"Calculate Metrics\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def calculate_metrics_and_plot(change):\n",
    "    with output:\n",
    "        output.clear_output()  # Clear previous output\n",
    "        tp = tp_widget.value\n",
    "        fp = fp_widget.value\n",
    "        fn = fn_widget.value\n",
    "        tn = tn_widget.value\n",
    "\n",
    "        # Confusion Matrix and Metrics Calculation\n",
    "        total = tp + fp + fn + tn\n",
    "        y_true = np.array([1] * tp + [1] * fn + [0] * tn + [0] * fp)\n",
    "        y_pred = np.array([1] * tp + [0] * fn + [0] * tn + [1] * fp)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)  # Sensitivity\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "        # ROC and PR Calculations\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_true, y_pred)\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            roc_auc = None\n",
    "            fpr, tpr = None, None\n",
    "\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred)\n",
    "        pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "        # Plot Metrics\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "        # Bar Plot for Metrics\n",
    "        metrics = [\n",
    "            (\"Accuracy\", accuracy),\n",
    "            (\"Recall\", recall),\n",
    "            (\"Precision\", precision),\n",
    "            (\"F1 Score\", f1),\n",
    "            (\"Cohen's Kappa\", cohen_kappa),\n",
    "            (\"ROC AUC\", roc_auc if roc_auc is not None else 0),\n",
    "            (\"PR AUC\", pr_auc),\n",
    "        ]\n",
    "        metric_names, metric_values = zip(*metrics)\n",
    "        axs[0].bar(metric_names, metric_values, color=\"blue\")\n",
    "        axs[0].set_ylim(0, 1)\n",
    "        axs[0].set_title(\"Metrics Overview\")\n",
    "        axs[0].set_ylabel(\"Score\")\n",
    "        axs[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # ROC Curve\n",
    "        if fpr is not None and tpr is not None:\n",
    "            axs[1].plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.3f}\")\n",
    "            axs[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "            axs[1].set_title(\"ROC Curve\")\n",
    "            axs[1].set_xlabel(\"False Positive Rate\")\n",
    "            axs[1].set_ylabel(\"True Positive Rate\")\n",
    "            axs[1].legend()\n",
    "            axs[1].grid(alpha=0.7)\n",
    "        else:\n",
    "            axs[1].text(0.5, 0.5, \"ROC Curve not available\", ha=\"center\", va=\"center\")\n",
    "            axs[1].set_title(\"ROC Curve\")\n",
    "            axs[1].axis(\"off\")\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        axs[2].plot(recall_vals, precision_vals, label=f\"PR AUC = {pr_auc:.3f}\")\n",
    "        axs[2].set_title(\"Precision-Recall Curve\")\n",
    "        axs[2].set_xlabel(\"Recall\")\n",
    "        axs[2].set_ylabel(\"Precision\")\n",
    "        axs[2].legend()\n",
    "        axs[2].grid(alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print Confusion Matrix\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"|          | Predicted Positive | Predicted Negative |\")\n",
    "        print(f\"|----------|---------------------|---------------------|\")\n",
    "        print(f\"| Actual Positive | {tp:<19} | {fn:<19} |\")\n",
    "        print(f\"| Actual Negative | {fp:<19} | {tn:<19} |\")\n",
    "\n",
    "# Attach button click event\n",
    "button.on_click(calculate_metrics_and_plot)\n",
    "\n",
    "# Display button and output\n",
    "display(button, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2e9c4",
   "metadata": {},
   "source": [
    "# **Sigmoid Function and Decision Threshold in Prediction**\n",
    "\n",
    "In classification algorithms like logistic regression, a key component is the sigmoid function. It transforms the results of a linear decision function into probabilities, which can then be used to assign samples to classes using a decision threshold.\n",
    "\n",
    "---\n",
    "\n",
    "## **Sigmoid Function**\n",
    "\n",
    "The sigmoid function, denoted as $ \\sigma(z) $, is defined by the formula:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ z $ is the result of the linear function $ z = w^T x + b $, where $ w $ is the weight vector, $ x $ is the feature vector, and $ b $ is the bias.\n",
    "\n",
    "### **Properties of the Sigmoid Function:**\n",
    "1. **Range of Values**:\n",
    "   - The sigmoid function maps any value $ z $ to the range $ (0, 1) $, allowing it to be interpreted as a probability.\n",
    "\n",
    "2. **Shape**:\n",
    "   - The sigmoid function has an \"S\" shape and passes through the point $ (0, 0.5) $.\n",
    "   - For $ z \\to +\\infty $: $ \\sigma(z) \\to 1 $.\n",
    "   - For $ z \\to -\\infty $: $ \\sigma(z) \\to 0 $.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - The value $ \\sigma(z) $ represents the probability of the sample belonging to the positive class ($ y = 1 $).\n",
    "\n",
    "---\n",
    "\n",
    "## **Prediction Using the Sigmoid Function**\n",
    "\n",
    "1. **Transforming Linear Outputs into Probabilities**:\n",
    "   - The model computes $ z = w^T x + b $ based on the input features $ x $.\n",
    "   - The sigmoid function transforms this value into a probability $ P(y=1|x) = \\sigma(z) $.\n",
    "\n",
    "2. **Example of the Sigmoid Function in Action**:\n",
    "   - If $ z = 0 $: $ \\sigma(z) = 0.5 $ (the sample is equally likely to belong to either class).\n",
    "   - If $ z > 0 $: $ \\sigma(z) > 0.5 $ (the model suggests the positive class).\n",
    "   - If $ z < 0 $: $ \\sigma(z) < 0.5 $ (the model suggests the negative class).\n",
    "\n",
    "---\n",
    "\n",
    "## **Decision Threshold**\n",
    "\n",
    "The decision threshold is the value that determines how the model assigns samples to classes based on the probabilities generated by the sigmoid function.\n",
    "\n",
    "### **How the Decision Threshold Works:**\n",
    "1. **Default Threshold**:\n",
    "   - The most commonly used threshold is $ 0.5 $. If $ P(y=1|x) \\geq 0.5 $, the model classifies the sample as belonging to the positive class ($ y = 1 $), otherwise to the negative class ($ y = 0 $).\n",
    "\n",
    "2. **Modifying the Threshold**:\n",
    "   - In some problems, adjusting the threshold can tailor the model to specific tasks:\n",
    "     - **Increasing the Threshold** (e.g., $ 0.7 $):\n",
    "       - The model becomes more conservative in assigning samples to the positive class.\n",
    "       - Higher precision but lower recall.\n",
    "     - **Decreasing the Threshold** (e.g., $ 0.3 $):\n",
    "       - The model assigns more samples to the positive class.\n",
    "       - Higher recall but lower precision.\n",
    "\n",
    "3. **Example Applications**:\n",
    "   - **Disease Diagnosis**:\n",
    "     - A low threshold (e.g., $ 0.3 $) is used to minimize the number of missed disease cases (false negatives).\n",
    "   - **Spam Detection**:\n",
    "     - A higher threshold (e.g., $ 0.7 $) reduces the number of false alarms (false positives).\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "1. The sigmoid function transforms model outputs into probabilities, enabling interpretation and decision-making.\n",
    "2. The decision threshold defines the boundary above which a sample is classified as positive. Its selection depends on the specific problem and preferences for precision and recall.\n",
    "3. Understanding the sigmoid function and the role of the decision threshold is crucial for interpreting results and tailoring classification models to various tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b195687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x22f440ffdc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-5,5,100)\n",
    "ys = [1 / (1+np.exp(-z)) for z in xs]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(xs,ys)\n",
    "plt.axvline(0,0,c='k',ls='--')\n",
    "plt.axhline(0.5,0,c='r',ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bfabe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84614e39194af686b809ade5fdf8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Regularization)', max=2.0, min=-2.0), FloatSliâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_classification_metrics(C=1.0, threshold=0.5)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate an imbalanced dataset (skrajny przypadek)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.92, 0.08],  # Imbalanced classes (95% to 5%)\n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Interactive function for classification metrics\n",
    "def interactive_classification_metrics(C=1.0, threshold=0.5):\n",
    "    # Train logistic regression model\n",
    "    model = LogisticRegression(C=C, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "    # Apply threshold\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot decision boundary and data points\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Mesh grid for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, levels=[0, threshold, 1], alpha=0.8, cmap='coolwarm', linestyles='dashed')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired, alpha=0.6)\n",
    "    plt.title(f\"Decision Boundary (Threshold={threshold:.2f})\\nAccuracy={acc:.2f}, Precision={precision:.2f}, Recall={recall:.2f}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.colorbar(label=\"Probability of Class 1\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for parameters\n",
    "interact(\n",
    "    interactive_classification_metrics,\n",
    "    C=widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='C (Regularization)'),\n",
    "    threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.5, description='Threshold')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afac3af",
   "metadata": {},
   "source": [
    "# **Class Imbalance in Classification Problems**\n",
    "\n",
    "Class imbalance refers to a situation where the number of samples in one class significantly differs from the number of samples in other classes within a dataset. This is a common issue in classification tasks, potentially leading to challenges in building effective models.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Class Imbalance?**\n",
    "\n",
    "In binary classification, class imbalance occurs when one class (e.g., the positive class) is represented by significantly fewer samples than the other class (e.g., the negative class). The class with more samples is called the **majority class**, and the class with fewer samples is called the **minority class**.\n",
    "\n",
    "### **Examples of Class Imbalance:**\n",
    "- **Fraud Detection**:\n",
    "  - 99.9% of transactions are legitimate, while only 0.1% are fraudulent.\n",
    "- **Medical Diagnosis**:\n",
    "  - Most individuals in a study are healthy, with only a small fraction being sick.\n",
    "- **Spam Filters**:\n",
    "  - The majority of emails are regular messages, while only a small percentage is spam.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is Class Imbalance a Problem?**\n",
    "\n",
    "1. **Model Bias Towards the Majority Class**:\n",
    "   - Machine learning algorithms optimize the objective function assuming balanced classes.\n",
    "   - The model might ignore the minority class, achieving high accuracy but failing to classify minority samples correctly.\n",
    "\n",
    "2. **Inadequate Evaluation Metrics**:\n",
    "   - **Accuracy** can be misleading for imbalanced datasets.\n",
    "     - Example: In a dataset where 99% of the data belongs to the majority class, a model predicting only the majority class achieves 99% accuracy but fails entirely for the minority class.\n",
    "\n",
    "3. **Insufficient Information in the Minority Class**:\n",
    "   - A small number of samples in the minority class limits the model's ability to learn patterns associated with that class.\n",
    "\n",
    "---\n",
    "\n",
    "## **How to Handle Class Imbalance**\n",
    "\n",
    "### **1. Data Transformation**\n",
    "- **Oversampling**:\n",
    "  - Create additional samples for the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "- **Undersampling**:\n",
    "  - Reduce the number of majority class samples by randomly removing a portion of them.\n",
    "- **Synthetic Data Generation**:\n",
    "  - Use generative models to create artificial samples for the minority class.\n",
    "\n",
    "### **2. Algorithm Modification**\n",
    "- **Class Weights**:\n",
    "  - Adjust the algorithm to penalize misclassifications of the minority class more heavily (e.g., `class_weight` in scikit-learn).\n",
    "- **Cost-sensitive Learning**:\n",
    "  - Use a cost function that incorporates different weights for classes.\n",
    "\n",
    "### **3. Choosing the Right Evaluation Metrics**\n",
    "- **Precision** and **Recall**:\n",
    "  - Crucial metrics for imbalanced datasets.\n",
    "- **F1-Score**:\n",
    "  - The harmonic mean of precision and recall, suitable for imbalanced data.\n",
    "- **ROC-AUC**:\n",
    "  - Evaluates the model's ability to distinguish between classes, independent of the decision threshold.\n",
    "- **PR Curve (Precision-Recall Curve)**:\n",
    "  - Illustrates the trade-off between precision and recall at different thresholds and is especially useful for imbalanced datasets.\n",
    "\n",
    "### **4. Adjusting the Decision Threshold**\n",
    "- Modify the threshold at which samples are classified as positive to account for the class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of Class Imbalance Impact**\n",
    "\n",
    "Consider a dataset where 95% of samples belong to the negative class and only 5% to the positive class:\n",
    "- A model predicting only the negative class achieves 95% accuracy but fails to identify any positive samples.\n",
    "- Focusing on metrics like precision, recall, and F1-score provides a better evaluation of the model's performance on the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- Class imbalance is a common issue that affects the effectiveness of classification models.\n",
    "- Proper data transformation, algorithm adjustments, and the use of appropriate evaluation metrics can help address this challenge.\n",
    "- Understanding which errors are more costly in a given problem is key to tailoring the modeling strategy to meet those needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2826761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b41c4b875474640a3935546cb7f0fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Regularization)', max=2.0, min=-2.0), FloatSliâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_classification_metrics(C=1.0, threshold=0.5)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Generate an imbalanced dataset (skrajny przypadek)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.92, 0.08],  # Imbalanced classes (95% to 5%)\n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Interactive function for classification metrics\n",
    "def interactive_classification_metrics(C=1.0, threshold=0.5):\n",
    "    # Train logistic regression model\n",
    "    model = LogisticRegression(C=C, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "    \n",
    "    # Apply threshold\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot decision boundary and data points\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Mesh grid for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, levels=[0, threshold, 1], alpha=0.8, cmap='coolwarm', linestyles='dashed')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired, alpha=0.6)\n",
    "    plt.title(f\"Decision Boundary (Threshold={threshold:.2f})\\nAccuracy={acc:.2f}, Precision={precision:.2f}, Recall={recall:.2f}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.colorbar(label=\"Probability of Class 1\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for parameters\n",
    "interact(\n",
    "    interactive_classification_metrics,\n",
    "    C=widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='C (Regularization)'),\n",
    "    threshold=widgets.FloatSlider(min=0.0, max=1.0, step=0.05, value=0.5, description='Threshold')\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
